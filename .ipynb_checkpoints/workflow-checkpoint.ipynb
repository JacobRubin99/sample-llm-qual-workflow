{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "859fa304-bb81-48ee-959b-e701b9a47a4b",
   "metadata": {},
   "source": [
    "# LLM Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d1bec5-21e1-429d-9153-64e45be86e70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b479292-5172-4eff-8300-e100a1528361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# import helper functions from helper functions notebook\n",
    "%run helper_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ada51a-6cda-4ecc-9251-cd91dbbf3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHORIZATION_TOKEN = os.getenv('AUTHORIZATION_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6077800-7d2f-4bf6-9a6a-48cfa23f578e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Define Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec62d69-3f69-44c2-a29d-cdebb06718bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which models we are using\n",
    "engines = {\n",
    "    \"sonnet-4\": \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "    \"gpt-5\": \"gpt-5\",\n",
    "    \"gpt-41-mini\": \"gpt-4.1-mini\",\n",
    "}\n",
    "\n",
    "reasoning_cols = [\"Engagement with Evidence\", \"Goal Orientation\",\n",
    "                  \"Collaborative Decision-Making\", \"Strategic Planning\"]\n",
    "hire_cols = [\"Cultural Responsiveness\", \"Parent/Community Engagement\", \"Academic Achievement\", \n",
    "                \"Candidate Experience/Expertise\", \"Evaluation\", \"School Culture Fit\"]\n",
    "\n",
    "cols =  reasoning_cols + hire_cols\n",
    "\n",
    "model_order = [\n",
    "    \"GPT-5\",\n",
    "    \"GPT-4.1 mini\",\n",
    "    \"Claude Sonnet 4\",\n",
    "]\n",
    "color_map = {\n",
    "    \"GPT-5\": \"#426737\",\n",
    "    \"GPT-4.1 mini\": \"#bab97d\",\n",
    "    \"Claude Sonnet 4\": \"#f9b40d\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ea37a-6270-45a1-880a-e1fb1a7d40ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 1: Prepare Rubrics for Human Scorers and LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c61be-39c6-43f9-a724-c0a66e08fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rubrics from Amazon S3 bucket\n",
    "rubric_map = {\n",
    "    \"zero-shot\": [\"jacob.m.rubin@vanderbilt.edu/2025-12-16/c4eba566-840f-4ee1-9158-beb03b013b6f.json\"],\n",
    "    \"examples\": [\"jacob.m.rubin@vanderbilt.edu/2025-12-17/da43579e-2f77-4aeb-90e3-4015ba9a75d8.json\"],\n",
    "    \"cot_examples\": [\"jacob.m.rubin@vanderbilt.edu/2025-12-17/b28011bf-d14c-46d7-8584-be51360d4ecf.json\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b483dbec-71d2-49a0-88c7-0ae584bba945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build sample payload from Vanderbilt Amplify\n",
    "\n",
    "url = \"https://prod-api.vanderbilt.ai/chat\"\n",
    "\n",
    "def build_payload(message_content, prompt=\"Case Study Scoring\", model_engine=engines[\"gpt-5\"], codebook_type=\"zero-shot\"):\n",
    "    data_sources = rubric_map[codebook_type]\n",
    "\n",
    "    payload = {\n",
    "        \"data\": {\n",
    "            \"model\": model_engine,\n",
    "            \"temperature\": 0,\n",
    "            \"max_tokens\": 4096,\n",
    "            \"dataSources\": data_sources,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": message_content\n",
    "                }\n",
    "            ],\n",
    "            \"options\": {\n",
    "                \"ragOnly\": False,\n",
    "                \"skipRag\": True,\n",
    "                \"model\": {\n",
    "                    \"id\": model_engine\n",
    "                },\n",
    "                \"assistantId\":\"\",\n",
    "                \"prompt\": prompt\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return json.dumps(payload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef8e72a-70b2-415f-8fda-31523e268cea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Verify Rubrics are Uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6e930-6e19-4fff-b078-d4b5cd4e69fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example API call\n",
    "payload = build_payload(\"What is the name of the document uploaded? Provide the full text that scores a 2 on Collaborative Decision-Making.\", prompt=\"Testing\", model_engine=engines[\"gpt-5\"], codebook_type=\"zero-shot\")\n",
    "\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'Authorization': f\"Bearer {AUTHORIZATION_TOKEN}\"\n",
    "}\n",
    "\n",
    "# response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "# response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a509b-576f-4c32-bf21-66abcef64466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask if the LLM knows what the uploaded codebook is\n",
    "content_message = \"Provide the full list of labels that you will score with using the Rubric Word Document.\"\n",
    "\n",
    "payload = build_payload(content_message, prompt=\"Testing file name\", model_engine=engines[\"gpt-5\"], codebook_type=\"zero-shot\")\n",
    "\n",
    "# response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "# response_json = response.json()\n",
    "# raw_json = response_json[\"data\"]\n",
    "# raw_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787400b0-d8e1-4eca-a5cd-e8fcf5029715",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_message = \"Using the Rubric document, what are the possible values to score for Cultural Responsiveness? What about Engagement with Evidence?\"\n",
    "\n",
    "payload = build_payload(content_message, prompt=\"Testing cultural responsiveness\", model_engine=engines[\"gpt-41-mini\"])\n",
    "\n",
    "# response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "# response_json = response.json()\n",
    "# raw_json = response_json[\"data\"]\n",
    "# raw_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9eca5-9c0f-4699-88f9-d1392c5cc93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_message = \"Using the Rubric document, give me the full text of the examples under the category Strategic Planning.\"\n",
    "\n",
    "payload = build_payload(content_message, prompt=\"Testing\", model_engine=engines[\"gpt-41-mini\"], codebook_type=\"cot_examples\")\n",
    "\n",
    "# response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "# response_json = response.json()\n",
    "# raw_json = response_json[\"data\"]\n",
    "# raw_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da824243-de80-4e20-9b93-634b13d29b34",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 2: Split the Dataset and Test LLMs' Basic Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec44c26-9f09-400b-94de-90c7f5fe543f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c28e577-3adc-4cb9-befc-d78118e40e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in response data\n",
    "df = pd.read_csv('../../data/Case Study/case_study_responses.csv')\n",
    "df = df[df['response'].notna() & (df['response'].str.strip() != \"\")]\n",
    "df = df[[\"record_id\", \"response\"]]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277af541-b760-4609-a66f-f70759743ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train and test datasets\n",
    "SEED = 42\n",
    "\n",
    "# Take a random 75% sample for the test set\n",
    "test_df = df.sample(frac=0.75, random_state=SEED)\n",
    "\n",
    "# Use the remaining 25% for the training set\n",
    "train_df = df.drop(test_df.index)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7732b8-4739-48bd-a1bb-b5e43bc190b4",
   "metadata": {},
   "source": [
    "### Calculate Number of Scoring Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7f72ed-a0c8-4970-8c99-98de5b25359a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Define Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda6d2d1-ccb0-4ed6-9e84-79be21df15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_noconf = '''\n",
    "    Output the assessments using JSON with the following format:\n",
    "    [\n",
    "        {{\"record_id\": \"<unique id for response>\", \"Scorer Name\": \"{engine}\",\n",
    "            \"Goal Orientation\": \"<response>\", \"Engagement with Evidence\": \"<response>\", ..., \"Cultural Responsiveness\": \"<response>\", ...\n",
    "        }}, \n",
    "        ...\n",
    "    ]\n",
    "    '''\n",
    "\n",
    "content_conf = '''\n",
    "    Please give a confidence score on a scale of 0 to 1 for each predicted score.\n",
    "    \n",
    "    Output the assessments using JSON with the following format:\n",
    "    [\n",
    "        {{\"record_id\": \"<unique id for response>\", \"Scorer Name\": \"{engine}\",\n",
    "            \"Goal Orientation\": \"<response>\", \"Confidence Score for Goal Orientation\": \"<response>\", \"Engagement with Evidence\": \"<response>\", \"Confidence Score for Engagement with Evidence\": \"<response>\", ...  \n",
    "        }}, \n",
    "        ...\n",
    "    ]\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194aef9-8d00-4da3-b336-078ec94c845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_content_message(response_list, engine=\"gpt-5\", hasExamples=\"\", content_output_structure=content_noconf):\n",
    "    content_output_structure = content_output_structure.format(engine=engine)\n",
    "\n",
    "    content_message = f'''I will provide you with a series of responses to a question prompt, and you will use the instructions from the Rubric Word Document to score it with the provided labels. Here is the prompt:\n",
    "    \"It is the beginning of your first school year as principal of Booker T. Washington High/Elementary School, which has an enrollment of 1003/302 students. Below, we will ask you about your approach to leading in three different areas for this first school year. To help you answer these questions, the district has compiled some information about the school for you in an Excel spreadsheet. The spreadsheet contains the following: The first tab is a Staffing Report with summary information about your teachers. For simplicity, it only includes data on math and ELA / K-3 teachers. The next eight tabs, colored blue, contain short summaries from last year's evaluation information for each math and ELA / K-3 teacher. The remaining tabs summarize:\n",
    "    Student achievement data, Student enrollment data, Data from recent teacher and student surveys, and Data on teacher turnover. \n",
    "    \n",
    "    Two of your teachers have already indicated that they probably will leave at the end of this school year. Patrick Houser, a Geometry/1st grade teacher, has said he intends to retire. Barbara Nelson, an English III/3rd grade teacher, plans to move out of state to be closer to her aging parents. It's too early to know about others. \n",
    "    Given enrollment projections, you also anticipate needing an additional English/Kindergarten teacher in the following school year. \n",
    "    Given what you know now, describe how you would approach teacher retention this year. Then, begin mapping out an approach to teacher hiring and placement for the following year. Discuss the reasoning behind your approach: \n",
    "    What are your goals? How are you achieving those goals?\"\n",
    "\n",
    "    The responses are in a dictionary object with the format <record_id> : <response>. Here are the responses:\n",
    "    {response_list}\n",
    "\n",
    "    {content_output_structure}\n",
    "    \n",
    "    The labels to output are defined with \"Label\" in the rubric document. For example, \"Label: Goal Orientation\", \"Label: Engagement with Evidence\", etc.\n",
    "    The instructions on how to score each label are provided in the uploaded codebook document. For example, for the \"Goal Orientation\" item, you will use the \"Instructions: Only score 1 through 4. Score a 4 if respondent identifies goals...\". So the value for the \"Goal Orienation\" item will only be a number between 1 and 4 (never 0 or greater than 4).\n",
    "    {hasExamples}\n",
    "    Only output JSON. Do not output anything else. \n",
    "    '''\n",
    "    return content_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d883b8-ee1b-415c-bbcf-2cb83eb525a1",
   "metadata": {},
   "source": [
    "#### Define API Call to Amplify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623d370d-0b08-4621-9e1d-dd159178c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_amplify_api(df=train_df, engine_key=\"gpt-5\", iterations=1, chunk_size=8, \n",
    "                     call_type=\"by_item\", file_ext=\"05142025\", codebook_type=\"zero-shot\", \n",
    "                     hasConfidenceScores=False):\n",
    "    # Define headers\n",
    "    headers = {\n",
    "      'Content-Type': 'application/json',\n",
    "      'Authorization': f\"Bearer {AUTHORIZATION_TOKEN}\"\n",
    "    }\n",
    "    \n",
    "    all_dfs = []\n",
    "    # Make API calls\n",
    "    engine = engines[engine_key]\n",
    "    \n",
    "    # Number of scoring iterations\n",
    "    for j in range(0, iterations):\n",
    "        \n",
    "        # Remove rows where response is nan or an empty string\n",
    "        df = df[df.iloc[:, 1].notna() & (df.iloc[:, 1].astype(str).str.strip() != \"\")]\n",
    "        # Randomize row order\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "        # Randomize prompt name\n",
    "        random_number = random.random()\n",
    "        prompt = f'Testing Prompt: {random_number}'\n",
    "        print(\"Prompt: \", prompt, \"Call iteration: \", j)\n",
    "        \n",
    "        # Breaking up scoring because of JSON token limitations\n",
    "        for i in range(0, len(df)//chunk_size + 1):\n",
    "    \n",
    "            print(\"Scoring responses: \", i*chunk_size, \"through\", i*chunk_size+chunk_size)\n",
    "        \n",
    "            # API only allows max 4096 tokens, so only grade chunk_size responses at a time.\n",
    "            sample_df = df[chunk_size*i:chunk_size*i+chunk_size]    \n",
    "            data_dict = dict(zip(sample_df.iloc[:, 0], sample_df.iloc[:, 1]))\n",
    "\n",
    "            # Build prompt message\n",
    "            content = build_content_message(response_list=data_dict, engine=engine, hasExamples=(\n",
    "                \"Use the examples to help guide scoring.\" if codebook_type != \"zero-shot\" else \"\"),\n",
    "                                            content_output_structure=(content_conf if hasConfidenceScores \n",
    "                                                                      else content_noconf),\n",
    "                                           )\n",
    "            \n",
    "            payload = build_payload(content, prompt=prompt, model_engine=engine, codebook_type=codebook_type)\n",
    "\n",
    "            try: \n",
    "                # Make API call\n",
    "                response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "            \n",
    "                # Load json response into a dataframe\n",
    "                response_json = response.json()\n",
    "                raw_json = response_json[\"data\"]\n",
    "                cleaned_json = raw_json.strip('```json\\n').strip('```')\n",
    "                cleaned_json = cleaned_json.replace('\\\\_', '_')\n",
    "\n",
    "                if not raw_json:  # Empty or missing\n",
    "                    print(f\"Warning: Empty 'data' field in response on call {j}, chunk {i}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    data = json.loads(cleaned_json)          \n",
    "                    llm_df = pd.DataFrame(data)\n",
    "                    llm_df['call_number'] = j  # Add call number column\n",
    "                    llm_df['prompt_name'] = prompt  # Add prompt name column\n",
    "                    all_dfs.append(llm_df)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing JSON on call number {j}: {e}\")\n",
    "                    print(\"Response JSON: \", response_json)\n",
    "                    continue  # Skip this iteration of the for loop\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error on request or response parsing at call {j}, chunk {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Make the directory path if it doesn't exist\n",
    "    output_dir = f\"{call_type}/{engine_key}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Build base file path\n",
    "    base_filename = f\"{engine_key}_{file_ext}.csv\"\n",
    "    file_path = os.path.join(output_dir, base_filename)\n",
    "\n",
    "    # If file exists, add _1, _2, _3...\n",
    "    counter = 1\n",
    "    while os.path.exists(file_path):\n",
    "        file_path = os.path.join(output_dir, f\"{engine_key}_{file_ext}_{counter}.csv\")\n",
    "        counter += 1\n",
    "    \n",
    "    # Save file\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    final_df.to_csv(file_path, index=False)\n",
    "\n",
    "    return all_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799873c0-bd88-4075-ae90-9da897562cd7",
   "metadata": {},
   "source": [
    "#### Call API for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0fa264-0679-40a5-9b37-49b1329e32ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_types = ['zero-shot']\n",
    "\n",
    "engines_to_run = {\n",
    "    # \"sonnet-4\": \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "    # \"gpt-5\": \"gpt-5\",\n",
    "    # \"gpt-41-mini\": \"gpt-4.1-mini\",\n",
    "}\n",
    "\n",
    "for codebook_type in codebook_types:\n",
    "    for engine_name, engine_key in engines_to_run.items():\n",
    "        print(f\"Running {engine_name} with {codebook_type}...\")\n",
    "        res = call_amplify_api(\n",
    "            df=train_df,\n",
    "            engine_key=engine_name,\n",
    "            iterations=50,\n",
    "            chunk_size=20,\n",
    "            call_type=f'scores/train/{codebook_type}',\n",
    "            file_ext=f\"12162025\",\n",
    "            codebook_type=codebook_type\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a9c49d-ff91-4f12-96e6-d814e4873c07",
   "metadata": {},
   "source": [
    "#### Create RMSE Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f507a5da-2ac0-415d-9081-6afa40a8acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_5_df = concat_raw_scores(\"gpt-5\", path=f'scores/training_data/zero-shot/')\n",
    "sonnet_4_df = concat_raw_scores(\"sonnet-4\", path=f'scores/training_data/zero-shot/')\n",
    "gpt_41_mini_df = concat_raw_scores(\"gpt-41-mini\", path=f'scores/training_data/zero-shot/')\n",
    "\n",
    "training_df = pd.concat([gpt_5_df, sonnet_4_df, gpt_41_mini_df])\n",
    "\n",
    "# Convert values to numerics\n",
    "training_df = convert_vals_numeric(training_df)\n",
    "\n",
    "training_df = handle_invalid_values(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96040402-aa13-442d-9f11-32f203d70256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration to create the RMSE curve ---\n",
    "model_name_map = {\n",
    "    \"us.anthropic.claude-sonnet-4-20250514-v1:0\": 'Claude Sonnet 4',\n",
    "    \"gpt-4.1-mini\": \"GPT-4.1 mini\",\n",
    "    \"gpt-5\": \"GPT-5\",\n",
    "}\n",
    "\n",
    "reasoning_cols = [\"Engagement with Evidence\", \"Goal Orientation\",\n",
    "                  \"Collaborative Decision-Making\", \"Strategic Planning\"]\n",
    "hire_cols = [\"Cultural Responsiveness\", \"Parent/Community Engagement\", \"Academic Achievement\", \n",
    "                \"Candidate Experience/Expertise\", \"Evaluation\", \"School Culture Fit\"]\n",
    "\n",
    "column_groups = {\n",
    "    \"Reasoning\": reasoning_cols,\n",
    "    \"Hiring Priorities\": hire_cols\n",
    "}\n",
    "\n",
    "# Parameters\n",
    "max_sample = 50\n",
    "n_iterations = 50\n",
    "\n",
    "# Map model names\n",
    "training_df[\"Scorer Name Clean\"] = training_df[\"Scorer Name\"].map(model_name_map)\n",
    "\n",
    "llms = ['GPT-5', \n",
    "        'GPT-4.1 mini',\n",
    "        'Claude Sonnet 4',\n",
    "       ]\n",
    "\n",
    "# --- Function to compute RMSE curve for one column ---\n",
    "def compute_rmse_curve(values, max_sample=50, n_iterations=50):\n",
    "    \"\"\"Compute RMSE vs. sub-sample size for a single column.\"\"\"\n",
    "    values = values[~np.isnan(values)]\n",
    "    overall_mean = np.nanmean(values)\n",
    "    rmse_curve = []\n",
    "    for sample_size in range(1, max_sample + 1):\n",
    "        sample_means = []\n",
    "        for _ in range(n_iterations):\n",
    "            sample = np.random.choice(values, size=sample_size, replace=True)\n",
    "            sample_means.append(np.mean(sample))\n",
    "        rmse = np.sqrt(np.mean((np.array(sample_means) - overall_mean) ** 2))\n",
    "        rmse_curve.append(rmse)\n",
    "    return np.array(rmse_curve)\n",
    "\n",
    "# --- Compute RMSE curves by LLM and group ---\n",
    "results_by_group = {}\n",
    "\n",
    "for group_name, cols in column_groups.items():\n",
    "    group_results = {llm: [] for llm in llms}\n",
    "    \n",
    "    for llm in llms:\n",
    "        df_llm = training_df[training_df[\"Scorer Name Clean\"] == llm]\n",
    "        \n",
    "        # Compute RMSE curve for each column, then average across columns\n",
    "        item_curves = []\n",
    "        for col in cols:\n",
    "            if col in df_llm.columns:\n",
    "                values = df_llm[col].dropna().values\n",
    "                if len(values) > 0:\n",
    "                    item_curves.append(compute_rmse_curve(values, max_sample, n_iterations))\n",
    "        \n",
    "        # Average RMSE curves (if at least one item present)\n",
    "        if item_curves:\n",
    "            mean_curve = np.nanmean(item_curves, axis=0)\n",
    "            group_results[llm] = mean_curve\n",
    "        else:\n",
    "            group_results[llm] = np.full(max_sample, np.nan)\n",
    "    \n",
    "    results_by_group[group_name] = group_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a390aa2c-cf35-4c95-b2dc-47aa187927a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "\n",
    "# --- Plot three subplots side-by-side ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6), sharey=True)\n",
    "\n",
    "elbows_summary = {}  # store elbow per group per LLM\n",
    "\n",
    "for ax, (group_name, rmse_results) in zip(axes, results_by_group.items()):\n",
    "    elbows_summary[group_name] = {}\n",
    "    \n",
    "    for llm in llms:\n",
    "        y = rmse_results[llm]\n",
    "        x = np.arange(1, max_sample + 1)\n",
    "\n",
    "        # Plot RMSE curve\n",
    "        ax.plot(x, y, color=color_map[llm], label=llm, linewidth=2)\n",
    "\n",
    "        # Detect elbow (only if no NaNs and curve is valid)\n",
    "        if np.all(np.isfinite(y)) and len(np.unique(y)) > 3:\n",
    "            kneedle = KneeLocator(\n",
    "                x, y, curve=\"convex\", direction=\"decreasing\",\n",
    "                \n",
    "            )\n",
    "            elbow = kneedle.knee\n",
    "\n",
    "            if elbow is not None:\n",
    "                elbows_summary[group_name][llm] = elbow\n",
    "                ax.axvline(elbow, color=color_map[llm], linestyle=\"--\", alpha=0.6)\n",
    "                ax.text(\n",
    "                    elbow, np.nanmin(y) + (np.nanmax(y) - np.nanmin(y)) * 0.05,\n",
    "                    f\"{elbow:.0f}\", color=color_map[llm],\n",
    "                    rotation=90, va=\"bottom\", ha=\"right\", fontsize=8\n",
    "                )\n",
    "            else:\n",
    "                elbows_summary[group_name][llm] = np.nan\n",
    "        else:\n",
    "            elbows_summary[group_name][llm] = np.nan\n",
    "    \n",
    "    ax.set_title(group_name)\n",
    "    ax.set_xlabel(\"# of Scoring Iterations\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Average RMSE\")\n",
    "axes[-1].legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# plt.suptitle(\"Average RMSE vs. Sub-sample Size Across Rubric Categories\", fontsize=14)\n",
    "plt.tight_layout(rect=[0, 0, 0.95, 0.95])\n",
    "plt.savefig(\"./output/training/RMSE_iterations_plot_avg_rmse_by_group_with_kneedle.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "elbows_df = pd.DataFrame(elbows_summary).T\n",
    "display(elbows_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988dc9a4-0a3c-46e7-8d64-b2d099b8f8ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Concat Training Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef247a-1a26-4b18-8a2d-3f7ad77b9b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'training_data'\n",
    "\n",
    "gpt_5_df = concat_raw_scores(\"gpt-5\", path=f'scores/{dataset}/zero-shot/')\n",
    "sonnet_4_df = concat_raw_scores(\"sonnet-4\", path=f'scores/{dataset}/zero-shot/')\n",
    "gpt_41_mini_df = concat_raw_scores(\"gpt-41-mini\", path=f'scores/{dataset}/zero-shot/')\n",
    "\n",
    "iters_df = pd.concat([gpt_5_df, sonnet_4_df, gpt_41_mini_df])\n",
    "print('Number of unique records in testing_df', len(gpt_5_df['record_id'].unique()))\n",
    "\n",
    "# # Convert values to numerics\n",
    "iters_df = convert_vals_numeric(iters_df)\n",
    "    \n",
    "# # Handle invalid values\n",
    "iters_df = handle_invalid_values(iters_df)\n",
    "\n",
    "# filtered_df = concat_llm_human_df(iters_df, is_training_data=True)\n",
    "# filtered_df.to_csv('./scores/filtered/training_data_df.csv', index=False)\n",
    "# filtered_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0454bb-2bd7-492a-bdd0-e3f14eb4303f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 3: Run Zero-Shot, Few-Shot, In-Context-Learning (ICL) Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c25c7-03ea-4291-8e4e-a1093d6172af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Test Set Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e21db6-b2bc-44ea-adc9-34f5098eae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_types = ['zero-shot', 'examples', 'cot_examples']\n",
    "\n",
    "engines_to_run = {\n",
    "    # \"sonnet-4\": \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "    # \"gpt-41-mini\": \"gpt-4.1-mini\",\n",
    "    # \"gpt-5\": \"gpt-5\",\n",
    "}\n",
    "\n",
    "for codebook_type in codebook_types:\n",
    "    for engine_name, engine_key in engines_to_run.items():\n",
    "        print(f\"Running {engine_name} with {codebook_type}...\")\n",
    "        res = call_amplify_api(\n",
    "            df=test_df,\n",
    "            engine_key=engine_name,\n",
    "            iterations=9,\n",
    "            chunk_size=20,\n",
    "            call_type=f'scores/{codebook_type}',\n",
    "            file_ext=f\"12172025\",\n",
    "            codebook_type=codebook_type\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4313f667-4663-483c-8d4c-9fe94498a4f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Generate Uncertainty Scores Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478442e3-0bbc-4b9c-9cbb-b33c3b10b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_types = ['zero-shot', 'examples', \"cot_examples\"]\n",
    "\n",
    "engines_to_run = {\n",
    "    # \"sonnet-4\": \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "    # \"gpt-41-mini\": \"gpt-4.1-mini\",\n",
    "    # \"gpt-5\": \"gpt-5\",\n",
    "}\n",
    "\n",
    "for engine_name, engine_key in engines_to_run.items():\n",
    "    for codebook_type in codebook_types:\n",
    "        print(f\"Running {engine_name} with {codebook_type}...\")\n",
    "        res = call_amplify_api(\n",
    "            df=test_df,\n",
    "            engine_key=engine_name,\n",
    "            iterations=1,\n",
    "            chunk_size=10,\n",
    "            call_type=f'scores/uncertainty/{codebook_type}',\n",
    "            file_ext=f\"12172025\",\n",
    "            codebook_type=codebook_type,\n",
    "            hasConfidenceScores=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f609c-f56b-4a53-979f-d1074266ac7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 4: Evaluate Compliance, Variation, Consistency, Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcbc1ea-0a3a-4ea0-bb99-fbb6da33dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluation_workflow(codebook_type, valid_ids, calc_compliance=False):\n",
    "    # Concat Scores Data from all models\n",
    "    gpt_5_df = concat_raw_scores(\"gpt-5\", path=f'scores/{codebook_type}')\n",
    "    sonnet_4_df = concat_raw_scores(\"sonnet-4\", path=f'scores/{codebook_type}')\n",
    "    gpt_41_mini_df = concat_raw_scores(\"gpt-41-mini\", path=f'scores/{codebook_type}')\n",
    "\n",
    "    # Combine all data into singular dataframe\n",
    "    testing_df = pd.concat([gpt_5_df, sonnet_4_df,  gpt_41_mini_df])\n",
    "    testing_df = testing_df[~testing_df['record_id'].isin(training_ids)]\n",
    "\n",
    "    print('Number of unique records in testing_df', len(testing_df['record_id'].unique()))\n",
    "\n",
    "    # Convert values\n",
    "    testing_df = convert_vals_numeric(testing_df)\n",
    "\n",
    "    # Evaluate Compliance\n",
    "    compliance_df = []\n",
    "    if calc_compliance:\n",
    "        compliance_df = run_compliance_checks(testing_df, valid_ids, codebook_type)\n",
    "\n",
    "    # Handle invalid values\n",
    "    testing_df = handle_invalid_values(testing_df)\n",
    "\n",
    "    # Evaluate Variation\n",
    "    variation_df = []\n",
    "    variation_df = save_variation_df(testing_df, codebook_type)\n",
    "\n",
    "    # Evaluate Uncertainty\n",
    "    uncertainty_df = compute_entropy_pivot(\n",
    "        testset_df=build_testset_df(\n",
    "            codebook_types=[\"zero-shot\", \"examples\", \"cot_examples\"],\n",
    "            training_ids=training_ids,\n",
    "        ),\n",
    "        rubric_items=cols,\n",
    "        reasoning_cols=reasoning_cols,\n",
    "        hire_cols=hire_cols,\n",
    "        scorer_name_map={\n",
    "            \"us.anthropic.claude-sonnet-4-20250514-v1:0\": \"Claude 4 Sonnet\",\n",
    "            \"gpt-4.1-mini\": \"GPT-41-mini\",\n",
    "            \"gpt-5\": \"GPT-5\",\n",
    "        },\n",
    "        scorer_order=[\"GPT-5\", \"GPT-41-mini\", \"Claude 4 Sonnet\"],\n",
    "    )\n",
    "    \n",
    "    # Create singular dataframe for calculating ICC/consistency later\n",
    "    filtered_df = []\n",
    "    filtered_df = concat_llm_human_df(testing_df, rounded=False)\n",
    "    filtered_df.to_csv(f\"scores/filtered/{codebook_type}_df.csv\")\n",
    "\n",
    "    return {\n",
    "        \"compliance_df\": compliance_df,\n",
    "        \"variation_df\": variation_df,\n",
    "        \"uncertainty_df\": uncertainty_df,\n",
    "        \"filtered_df\": filtered_df\n",
    "    }\n",
    "\n",
    "# Run for all desired codebook types\n",
    "results = {}\n",
    "valid_ids = np.array(test_df[\"record_id\"])\n",
    "for codebook_type in [\"zero-shot\", \"examples\", \"cot_examples\"]:\n",
    "    results[codebook_type] = evaluation_workflow(codebook_type, valid_ids, calc_compliance = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
