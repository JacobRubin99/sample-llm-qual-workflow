{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f83dc786-8891-48d4-9b6c-36d5d57f9ad2",
   "metadata": {},
   "source": [
    "# IntraClass Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b12885-61ed-4e5c-8206-79ff6d21ff4d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8057cd9-de36-4375-8589-5c1c1dadfde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c93361e-9442-4cb1-a95f-e2bfaf6146ec",
   "metadata": {},
   "source": [
    "## Define Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49951940-c3c2-46f8-953f-f21e76c2e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rubric item categories\n",
    "reasoning_cols = [\"Engagement with Evidence\", \"Goal Orientation\",\n",
    "                  \"Collaborative Decision-Making\", \"Strategic Planning\"]\n",
    "hire_cols = [\n",
    "    \"Cultural Responsiveness\", \"Parent/Community Engagement\", \"Academic Achievement\",\n",
    "    \"Candidate Experience/Expertise\", \"Evaluation\", \"School Culture Fit\"\n",
    "]\n",
    "\n",
    "rubric_items = reasoning_cols + hire_cols\n",
    "\n",
    "# Set up data to remove \"human avg\" column, and add an extra column called \"Scoring Group\" that differs model from human scorers\n",
    "model_scorers = ['gpt-4.1-mini', 'gpt-5', \"us.anthropic.claude-sonnet-4-20250514-v1:0\"]\n",
    "human_scorers = ['JR', 'MF']\n",
    "human_avg_score = ['human avg']\n",
    "\n",
    "valid_scorer_names = {\n",
    "    \"us.anthropic.claude-sonnet-4-20250514-v1:0\": 'Claude Sonnet 4',\n",
    "    \"gpt-4.1-mini\": \"GPT-4.1 mini\",\n",
    "    \"gpt-5\": \"GPT-5\",\n",
    "    \"human\": \"Trained Annotators\",\n",
    "}\n",
    "\n",
    "\n",
    "codebook_types = [\"zero-shot\", \"examples\", \"cot_examples\"]\n",
    "\n",
    "codebook_type_names = {\n",
    "        \"zero-shot\": \"Zero-Shot\",\n",
    "        \"examples\": \"Few-Shot\",\n",
    "        \"cot_examples\": \"CoT\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80309cd-8e16-4c05-8e77-11d336502735",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0842a281-6ef5-4cbb-8035-89b627df4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for ctype in codebook_types:\n",
    "    df = pd.read_csv(\n",
    "        f\"./scores/filtered/{ctype}_df.csv\", \n",
    "        header=0, index_col=0\n",
    "    )\n",
    "    df[\"codebook type\"] = ctype\n",
    "    df.loc[df[\"Scorer Name\"].isin(human_scorers), \"codebook type\"] = \"human\"\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate codebook files\n",
    "scores_df = pd.concat(dfs, ignore_index=True).drop_duplicates()\n",
    "\n",
    "# Add \"Scoring Group\" column\n",
    "scores_df = scores_df[scores_df[\"Scorer Name\"] != \"human avg\"]\n",
    "scores_df.loc[scores_df[\"Scorer Name\"].isin(model_scorers), \"Scoring Group\"] = \"model\"\n",
    "scores_df.loc[scores_df[\"Scorer Name\"].isin(human_scorers), \"Scoring Group\"] = \"human\"\n",
    "\n",
    "scores_df['Scorer Name'].unique()\n",
    "# scores_df.to_csv('./scores/filtered dataframes/test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b2b7fc-0de6-4821-b73b-1220d384f9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db3e3ce1-d957-48ba-bdcc-a89542ef4074",
   "metadata": {},
   "source": [
    "## Calculate Human-Human ICC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3064973-6701-40b1-96e5-b0975d76c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_human_icc(scores_df):\n",
    "    \"\"\"\n",
    "    Compute ICC(2,1) for human scorers per rubric item, \n",
    "    and average ICCs for Reasoning and Hiring Priorities categories.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    scores_df : pd.DataFrame\n",
    "        DataFrame containing scores with columns 'record_id', 'Scorer Name', 'Scoring Group', and rubric items\n",
    "    rubric_items : list\n",
    "        List of all rubric items to compute ICC for\n",
    "    reasoning_cols : list\n",
    "        List of rubric items corresponding to Reasoning\n",
    "    hire_cols : list\n",
    "        List of rubric items corresponding to Hiring Priorities\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    icc_human_df : pd.DataFrame\n",
    "        DataFrame with ICC(2,1) and 95% CI for each rubric item\n",
    "    human_iccs : dict\n",
    "        Dictionary with averaged ICCs for 'Reasoning' and 'Hiring Priorities'\n",
    "    \"\"\"\n",
    "    \n",
    "    icc_human_results = []\n",
    "\n",
    "    for item in rubric_items:\n",
    "        # Filter for human scorers and the current item\n",
    "        group_human = scores_df[scores_df[\"Scoring Group\"] == \"human\"][[\"record_id\", \"Scorer Name\", item]].dropna()\n",
    "        group_human_long = group_human.rename(columns={item: \"score\", \"Scorer Name\": \"rater\"})\n",
    "        \n",
    "        # Only compute ICC if enough raters and records\n",
    "        if group_human_long[\"rater\"].nunique() > 1 and group_human_long[\"record_id\"].nunique() > 1:\n",
    "            icc_human = pg.intraclass_corr(\n",
    "                data=group_human_long, targets='record_id', raters='rater', ratings='score'\n",
    "            )\n",
    "            icc2 = icc_human[icc_human[\"Type\"] == \"ICC2\"]\n",
    "            if not icc2.empty:\n",
    "                icc_human_results.append({\n",
    "                    \"Rubric Item\": item,\n",
    "                    \"ICC(2,1)\": icc2[\"ICC\"].values[0],\n",
    "                    \"CI95%\": icc2[\"CI95%\"].values[0]\n",
    "                })\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    icc_human_df = pd.DataFrame(icc_human_results)\n",
    "\n",
    "    \n",
    "    # Compute averages by category\n",
    "    icc_human_reason_avg = icc_human_df.loc[\n",
    "        icc_human_df[\"Rubric Item\"].isin(reasoning_cols), \"ICC(2,1)\"\n",
    "    ].mean()\n",
    "    \n",
    "    icc_human_hire_avg = icc_human_df.loc[\n",
    "        icc_human_df[\"Rubric Item\"].isin(hire_cols), \"ICC(2,1)\"\n",
    "    ].mean()\n",
    "    \n",
    "    human_iccs = {\n",
    "        \"Reasoning\": icc_human_reason_avg,\n",
    "        \"Hiring Priorities\": icc_human_hire_avg\n",
    "    }\n",
    "    \n",
    "    return icc_human_df, human_iccs\n",
    "\n",
    "icc_human_df, human_iccs = compute_human_icc(scores_df)\n",
    "\n",
    "icc_human_df\n",
    "human_iccs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae3ecd-6a2d-4e59-a466-b1bfad629f11",
   "metadata": {},
   "source": [
    "## Calculate Model-Human ICC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914bf18e-2760-4763-b7a7-8dc62cdd4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_model_icc(scores_df):\n",
    "    \"\"\"\n",
    "    Compute ICC(2,1) for model raters against human averages, grouped by codebook type and rubric item.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scores_df : pd.DataFrame\n",
    "        DataFrame containing scores with columns 'record_id', 'Scorer Name', 'Scoring Group', 'codebook type', and rubric items\n",
    "    rubric_items : list\n",
    "        List of all rubric items to compute ICC for\n",
    "    codebook_types : list\n",
    "        List of codebook types to compute ICC for\n",
    "    valid_scorer_names : dict, optional\n",
    "        Mapping to rename raters\n",
    "    codebook_type_names : dict, optional\n",
    "        Mapping to rename codebook types\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    icc_summary_df : pd.DataFrame\n",
    "        DataFrame with ICC(2,1) and 95% CI per rater, rubric item, and codebook type\n",
    "    \"\"\"\n",
    "\n",
    "    all_icc_results = []\n",
    "\n",
    "    # Loop through each codebook type\n",
    "    for ctype in codebook_types:\n",
    "        df_sub = scores_df[(scores_df[\"codebook type\"] == ctype) | (scores_df[\"codebook type\"] == \"human\")]\n",
    "\n",
    "        # Loop through each rubric item\n",
    "        for item in rubric_items:\n",
    "\n",
    "            # Group Human averages per subject\n",
    "            group_human_means = (\n",
    "                df_sub[df_sub[\"Scoring Group\"] == \"human\"]\n",
    "                .groupby(\"record_id\")[item]\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                .rename(columns={item: \"group_human_avg\"})\n",
    "            )\n",
    "\n",
    "            # Group Model individual rater scores\n",
    "            group_model = df_sub[df_sub[\"Scoring Group\"] == \"model\"][[\"record_id\", \"Scorer Name\", item]].dropna()\n",
    "\n",
    "            # Merge Group Model with Group Human averages\n",
    "            merged = pd.merge(group_model, group_human_means, on=\"record_id\", how=\"inner\").dropna()\n",
    "\n",
    "            for rater in merged[\"Scorer Name\"].unique():\n",
    "                rater_data = merged[merged[\"Scorer Name\"] == rater][\n",
    "                    [\"record_id\", item, \"group_human_avg\"]\n",
    "                ].dropna()\n",
    "\n",
    "                # Melt to long format for pingouin\n",
    "                icc_data = rater_data.melt(\n",
    "                    id_vars=\"record_id\",\n",
    "                    value_vars=[item, \"group_human_avg\"],\n",
    "                    var_name=\"rater\",\n",
    "                    value_name=\"score\"\n",
    "                )\n",
    "\n",
    "                icc = pg.intraclass_corr(\n",
    "                    data=icc_data, targets=\"record_id\", raters=\"rater\", ratings=\"score\"\n",
    "                )\n",
    "                icc2 = icc[icc[\"Type\"] == \"ICC2\"]\n",
    "\n",
    "                all_icc_results.append({\n",
    "                    \"Codebook Type\": ctype,\n",
    "                    \"Rubric Item\": item,\n",
    "                    \"Rater\": rater,\n",
    "                    \"ICC(2,1)\": icc2[\"ICC\"].values[0],\n",
    "                    \"CI95%\": icc2[\"CI95%\"].values[0]\n",
    "                })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    icc_summary_df = pd.DataFrame(all_icc_results)\n",
    "\n",
    "    # Rename raters and codebook types\n",
    "    if valid_scorer_names:\n",
    "        icc_summary_df[\"Rater\"] = icc_summary_df[\"Rater\"].replace(valid_scorer_names)\n",
    "    if codebook_type_names:\n",
    "        icc_summary_df[\"Codebook Type\"] = icc_summary_df[\"Codebook Type\"].replace(codebook_type_names)\n",
    "\n",
    "    return icc_summary_df\n",
    "\n",
    "icc_summary_df = compute_model_icc(scores_df)\n",
    "\n",
    "# Save combined ICC results\n",
    "icc_summary_df.to_csv(\"output/consistency/icc/all_ICC_results.csv\", index=False)\n",
    "\n",
    "icc_summary_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773bcd37-6d34-417f-8328-cf35326acddf",
   "metadata": {},
   "source": [
    "### Create ICC Model-Human Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb871e17-6680-470d-b039-ddbe19b302ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_icc_comparison_df(icc_summary_df, human_iccs, top_level_order=None, second_level_order=None, round_decimals=2):\n",
    "    \"\"\"\n",
    "    Create a comparison DataFrame of ICCs per Rater, with multi-level columns for \n",
    "    Reasoning and Hiring Priorities and sub-columns for Codebook Types. Includes a row\n",
    "    for Trained Annotators using provided human ICCs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    icc_summary_df : pd.DataFrame\n",
    "        DataFrame with columns ['Rater', 'Codebook Type', 'Rubric Item', 'ICC(2,1)']\n",
    "    reasoning_cols : list\n",
    "        List of rubric items corresponding to Reasoning\n",
    "    hire_cols : list\n",
    "        List of rubric items corresponding to Hiring Priorities\n",
    "    human_iccs : dict\n",
    "        Dictionary with averaged ICCs for 'Reasoning' and 'Hiring Priorities'\n",
    "    top_level_order : list, optional\n",
    "        Desired order for top-level columns (default ['Reasoning', 'Hiring Priorities'])\n",
    "    second_level_order : list, optional\n",
    "        Desired order for second-level columns (codebook types)\n",
    "    round_decimals : int, optional\n",
    "        Number of decimals to round the final DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_final : pd.DataFrame\n",
    "        MultiIndex column DataFrame with ICC averages per rater and Trained Annotators\n",
    "    \"\"\"\n",
    "    \n",
    "    if top_level_order is None:\n",
    "        top_level_order = [\"Reasoning\", \"Hiring Priorities\"]\n",
    "    if second_level_order is None:\n",
    "        second_level_order = icc_summary_df[\"Codebook Type\"].unique()\n",
    "    \n",
    "    # Get unique raters and codebook types\n",
    "    raters = icc_summary_df[\"Rater\"].unique()\n",
    "    codebook_types = icc_summary_df[\"Codebook Type\"].unique()\n",
    "    \n",
    "    # Prepare a dict to store the data\n",
    "    data = {}\n",
    "\n",
    "    # Loop through each top-level category\n",
    "    for category, cols in [(\"Reasoning\", reasoning_cols), (\"Hiring Priorities\", hire_cols)]:\n",
    "        data[category] = {}\n",
    "        for ctype in codebook_types:\n",
    "            col_values = []\n",
    "            for rater in raters:\n",
    "                mask = (\n",
    "                    (icc_summary_df[\"Rater\"] == rater) &\n",
    "                    (icc_summary_df[\"Codebook Type\"] == ctype) &\n",
    "                    (icc_summary_df[\"Rubric Item\"].isin(cols))\n",
    "                )\n",
    "                icc_vals = icc_summary_df.loc[mask, \"ICC(2,1)\"]\n",
    "                col_values.append(icc_vals.mean())\n",
    "            data[category][ctype] = col_values\n",
    "\n",
    "    # Create MultiIndex columns\n",
    "    multi_cols = pd.MultiIndex.from_product([[\"Reasoning\", \"Hiring Priorities\"], codebook_types])\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_final = pd.DataFrame(\n",
    "        data = pd.concat([pd.DataFrame(data[\"Reasoning\"]), pd.DataFrame(data[\"Hiring Priorities\"])], axis=1).values,\n",
    "        index=raters,\n",
    "        columns=multi_cols\n",
    "    )\n",
    "\n",
    "    # Add the Trained Annotators row\n",
    "    human_row = []\n",
    "    for category in [\"Reasoning\", \"Hiring Priorities\"]:\n",
    "        for ctype in codebook_types:\n",
    "            human_row.append(human_iccs[category])\n",
    "    df_final.loc[\"Trained Annotators\"] = human_row\n",
    "\n",
    "    # Round values\n",
    "    df_final = df_final.round(round_decimals)\n",
    "\n",
    "    # Reorder columns if desired\n",
    "    if top_level_order and second_level_order:\n",
    "        multi_cols_ordered = pd.MultiIndex.from_product([top_level_order, second_level_order])\n",
    "        df_final = df_final.reindex(columns=multi_cols_ordered)\n",
    "\n",
    "    return df_final\n",
    "\n",
    "df_final = create_icc_comparison_df(\n",
    "    icc_summary_df,\n",
    "    top_level_order=[\"Reasoning\", \"Hiring Priorities\"],\n",
    "    second_level_order=[\"Zero-Shot\", \"Few-Shot\", \"CoT\"],\n",
    "    human_iccs=human_iccs\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "df_final.to_csv(\"output/consistency/icc/icc_comparison_df.csv\")\n",
    "\n",
    "df_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad73c4-6b89-4fce-a06e-f1fb0b50c47d",
   "metadata": {},
   "source": [
    "## Calculate Model-Human ICC with Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd52fbc-d059-4538-a2a1-48b356589ce1",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eb9407-7868-4957-8f93-eab70d74902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iterations_check scores\n",
    "iter_scores_df = pd.read_csv(f\"./scores/filtered/training_data_df.csv\")\n",
    "iter_scores_df[\"codebook type\"] = \"zero-shot\"\n",
    "iter_scores_df.loc[iter_scores_df[\"Scorer Name\"].isin(human_scorers), \"codebook type\"] = \"human\"\n",
    "\n",
    "# Add \"Scoring Group\" column\n",
    "iter_scores_df = iter_scores_df[iter_scores_df[\"Scorer Name\"] != \"human avg\"]\n",
    "iter_scores_df.loc[iter_scores_df[\"Scorer Name\"].isin(model_scorers), \"Scoring Group\"] = \"model\"\n",
    "iter_scores_df.loc[iter_scores_df[\"Scorer Name\"].isin(human_scorers), \"Scoring Group\"] = \"human\"\n",
    "\n",
    "iter_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c24b2c-ec4d-4d01-be87-b49690ced958",
   "metadata": {},
   "outputs": [],
   "source": [
    "icc_human_df, human_iccs = compute_human_icc(iter_scores_df)\n",
    "\n",
    "icc_summary_df = compute_model_icc(iter_scores_df)\n",
    "\n",
    "df_iterations = create_icc_comparison_df(\n",
    "    icc_summary_df,\n",
    "    top_level_order=[\"Reasoning\", \"Hiring Priorities\"],\n",
    "    second_level_order=[\"Zero-Shot\"],\n",
    "    human_iccs=human_iccs\n",
    ")\n",
    "\n",
    "\n",
    "df_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b6977-31a1-4ff8-b3cb-642cbd172829",
   "metadata": {},
   "outputs": [],
   "source": [
    "icc_human_df, human_iccs = compute_human_icc(scores_df)\n",
    "\n",
    "icc_summary_df = compute_model_icc(scores_df)\n",
    "\n",
    "df_iterations = create_icc_comparison_df(\n",
    "    icc_summary_df,\n",
    "    top_level_order=[\"Reasoning\", \"Hiring Priorities\"],\n",
    "    second_level_order=[\"Zero-Shot\"],\n",
    "    human_iccs=human_iccs\n",
    ")\n",
    "\n",
    "\n",
    "df_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c10ffa1-4c14-4cf6-adac-0e0d62685180",
   "metadata": {},
   "source": [
    "## Calculate Model-Human ICC by Subsample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bcdb82-0a29-49f3-9d02-417110bb1298",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc9dfc9-5a13-4941-b75e-40745191727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for ctype in codebook_types:\n",
    "    df = pd.read_csv(\n",
    "        f\"./scores/filtered dataframes/{ctype}_df.csv\", \n",
    "        header=0, index_col=0\n",
    "    )\n",
    "    df[\"codebook type\"] = ctype\n",
    "    df.loc[df[\"Scorer Name\"].isin(human_scorers), \"codebook type\"] = \"human\"\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate codebook files\n",
    "scores_df = pd.concat(dfs, ignore_index=True).drop_duplicates()\n",
    "\n",
    "# Add \"Scoring Group\" column\n",
    "scores_df = scores_df[scores_df[\"Scorer Name\"] != \"human avg\"]\n",
    "scores_df.loc[scores_df[\"Scorer Name\"].isin(model_scorers), \"Scoring Group\"] = \"model\"\n",
    "scores_df.loc[scores_df[\"Scorer Name\"].isin(human_scorers), \"Scoring Group\"] = \"human\"\n",
    "\n",
    "\n",
    "print(len(scores_df[\"record_id\"].unique()))\n",
    "scores_df.shape\n",
    "# number of unique IDs (94) * number of models (6) * number of codebooks (3) + humans (94*2) = 1880 rows\n",
    "\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da253dff-fed1-4271-82a7-f8201e7eb356",
   "metadata": {},
   "source": [
    "### Calculate ICCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4349f8-ef25-469e-8067-6375c0919ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Parameters\n",
    "subsample_percents = range(4, 50, 2)  # 5%, 10%, ..., 100%\n",
    "num_repeats = 50  # number of subsampling repetitions per percentage\n",
    "\n",
    "# Function to compute ICCs for a given subsample\n",
    "def compute_iccs_for_subsample(df):\n",
    "    # These functions already handle codebook_type and rubric_item internally\n",
    "    icc_human_df, human_iccs = compute_human_icc(df)\n",
    "    print(human_iccs)\n",
    "    icc_summary_df = compute_model_icc(df)\n",
    "    df_iterations = create_icc_comparison_df(\n",
    "        icc_summary_df,\n",
    "        top_level_order=[\"Reasoning\", \"Hiring Priorities\"],\n",
    "        second_level_order=[\"Zero-Shot\", \"Few-Shot\", \"CoT\"],\n",
    "        human_iccs=human_iccs\n",
    "    )\n",
    "    return df_iterations\n",
    "\n",
    "# Collect results\n",
    "results = []\n",
    "\n",
    "for pct in subsample_percents:\n",
    "    n_ids = int(scores_df[\"record_id\"].nunique() * pct / 100)\n",
    "\n",
    "    for rep in range(num_repeats):\n",
    "        sampled_ids = (\n",
    "            scores_df[\"record_id\"]\n",
    "            .drop_duplicates()\n",
    "            .sample(n=n_ids, replace=False, random_state=rep)\n",
    "        )\n",
    "        subsample_df = scores_df[scores_df[\"record_id\"].isin(sampled_ids)]\n",
    "        print(\"PCT of subsamples: \", pct)\n",
    "        print(\"number of IDs: \", len(subsample_df[\"record_id\"].unique()))\n",
    "        \n",
    "        icc_df = compute_iccs_for_subsample(subsample_df)\n",
    "        icc_long = (\n",
    "            icc_df\n",
    "            .stack(level=[0, 1])  # stack both levels of columns\n",
    "            .reset_index()        # bring index and stacked levels into columns\n",
    "            .rename(columns={icc_df.index.name or 'index': 'Annotator', 0: 'ICC', 'level_1': 'Rubric', 'level_2': 'Codebook'})\n",
    "        )\n",
    "\n",
    "        icc_long[\"Subsample\"] = pct\n",
    "        icc_long[\"Repeat\"] = rep\n",
    "        results.append(icc_long)\n",
    "\n",
    "# Combine all results\n",
    "results_df = pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb99a83b-8f5c-4f47-81c5-09fcad45ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_ci(x):\n",
    "    mean = np.mean(x)\n",
    "    ci = 1.96 * np.std(x, ddof=1) / np.sqrt(len(x))\n",
    "    return pd.Series({'mean': mean, 'lower': mean - ci, 'upper': mean + ci})\n",
    "\n",
    "# Aggregate across repeats using apply\n",
    "agg_results = (\n",
    "    results_df\n",
    "    .groupby(['Subsample', 'level_0', 'Rubric', 'Codebook'])['ICC']\n",
    "    .apply(mean_ci)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "agg_results_wide = agg_results.pivot_table(\n",
    "    index=['Subsample', 'level_0', 'Rubric', 'Codebook'],\n",
    "    columns='level_4',\n",
    "    values='ICC'\n",
    ").reset_index()\n",
    "\n",
    "# agg_results\n",
    "agg_results_wide\n",
    "agg_results_wide.to_csv(\"output/consistency/icc/iteration_check_iccs_4_50_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95441bac-d67c-4c12-a318-22c64bfb9ec3",
   "metadata": {},
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d8d29-c9e9-4fac-8dcb-392dd81fadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results_wide = pd.read_csv(f\"./output/consistency/icc/iteration_check_iccs_4_50_2.csv\", header=0, index_col=0)\n",
    "agg_results_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc460a-8c5d-4311-b5a7-6d2b55771f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_order = [\n",
    "    \"GPT-5\",\n",
    "    \"GPT-4.1 mini\",\n",
    "    \"Claude Sonnet 4\",\n",
    "    \"Trained Annotators\"\n",
    "]\n",
    "color_map = {\n",
    "    \"GPT-5\": \"#426737\",\n",
    "    \"GPT-4.1 mini\": \"#bab97d\",\n",
    "    \"Claude Sonnet 4\": \"#f9b40d\",\n",
    "    \"Trained Annotators\": \"#000\",\n",
    "    \"Human Scorers\": \"#000\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d080ff85-cf70-4bdf-94ed-696950953b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_sample_iccs = pd.read_csv(f\"./output/consistency/icc/icc_comparison_df.csv\", header=[0, 1], index_col=0)\n",
    "\n",
    "full_sample_iccs_map = {\n",
    "    \"GPT-5\": 3,\n",
    "    \"GPT-4o\": 2,\n",
    "    \"GPT-4.1 mini\": 1,\n",
    "    \"Claude Sonnet 4\": 5,\n",
    "    \"Claude 3.5 Haiku\": 4,\n",
    "    \"Amazon Nova Lite\": 0,\n",
    "    \"Trained Annotators\": 6\n",
    "}\n",
    "\n",
    "# --- Compute \"within 10%\" per model-codebook-rubric combination ---\n",
    "summary_rows = []\n",
    "\n",
    "for rubric in agg_results_wide['Rubric'].unique():\n",
    "    for codebook in agg_results_wide['Codebook'].unique():\n",
    "\n",
    "        # Subset to this rubric & codebook\n",
    "        subset_rc = agg_results_wide[\n",
    "            (agg_results_wide['Rubric'] == rubric) &\n",
    "            (agg_results_wide['Codebook'] == codebook)\n",
    "        ]\n",
    "\n",
    "        # Loop over each model\n",
    "        for model in subset_rc['level_0'].unique():\n",
    "            full_icc = full_sample_iccs[(rubric, codebook)].iloc[full_sample_iccs_map[model]]\n",
    "            sub = subset_rc[subset_rc['level_0'] == model].copy()\n",
    "            sub['diff_pct'] = (sub['mean'] - full_icc).abs() / full_icc\n",
    "\n",
    "            within_10 = sub[sub['diff_pct'] <= 0.10]\n",
    "            within_5 = sub[sub['diff_pct'] <= 0.05]\n",
    "            min_subsample_10 = within_10['Subsample'].min()\n",
    "            min_subsample_5 = within_5['Subsample'].min()\n",
    "            summary_rows.append({\n",
    "                'Rubric': rubric,\n",
    "                'Codebook': codebook,\n",
    "                'Model': model,\n",
    "                'Full ICC': full_icc,\n",
    "                'Min Subsample (10%)': min_subsample_10,\n",
    "                'Min Subsample (5%)': min_subsample_5\n",
    "            })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "\n",
    "summary_df = summary_df[summary_df['Model'] != 'Trained Annotators']\n",
    "grouped_summary_df = summary_df.groupby(['Rubric', 'Codebook']).mean(['Min Subsample (10%)', 'Min Subsample (5%)'])\n",
    "grouped_summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdfe2e-9484-44c9-8f0b-a8eabe73de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use IndexSlice for multi-level indexing\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "combined_rows = []\n",
    "\n",
    "for rubric in grouped_summary_df.index.get_level_values('Rubric').unique():\n",
    "    # Select all codebooks for this rubric\n",
    "    subset = grouped_summary_df.loc[idx[rubric, ['CoT', 'Few-Shot', 'Zero-Shot']], :]\n",
    "    \n",
    "    # Compute mean across codebooks for this rubric\n",
    "    mean_full_icc = subset['Full ICC'].mean()\n",
    "    mean_min_10_sub = subset['Min Subsample (10%)'].mean()\n",
    "    mean_min_5_sub = subset['Min Subsample (5%)'].mean()\n",
    "    \n",
    "    # Append a tuple for MultiIndex\n",
    "    combined_rows.append(((rubric, 'Combined'), {'Full ICC': mean_full_icc,\n",
    "                                                 'Min Subsample (10%)': mean_min_10_sub,\n",
    "                                                 'Min Subsample (5%)': mean_min_5_sub}))\n",
    "\n",
    "# Convert to DataFrame and append\n",
    "combined_df = pd.DataFrame(\n",
    "    [row[1] for row in combined_rows],\n",
    "    index=pd.MultiIndex.from_tuples([row[0] for row in combined_rows],\n",
    "                                    names=grouped_summary_df.index.names)\n",
    ")\n",
    "\n",
    "grouped_summary_df = pd.concat([grouped_summary_df, combined_df]).sort_index()\n",
    "\n",
    "grouped_summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f15f4-2b53-4e78-bf16-00981dc01e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d566cadd-703a-4291-824b-3a6905b02216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_icc_subplots(data, codebook, color_map, model_order, show_CI=False, grouped_summary_df=None):\n",
    "    \n",
    "    rubric_order = [\"Reasoning\", \"Hiring Priorities\"]  \n",
    "    rubrics = [r for r in rubric_order if r in data['Rubric'].unique()]\n",
    "    n_rubrics = len(rubrics)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_rubrics, figsize=(6*n_rubrics, 6), sharey=True)\n",
    "    \n",
    "    if n_rubrics == 1:  # make sure axes is iterable\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, rubric in zip(axes, rubrics):\n",
    "        subset = data[data['Rubric'] == rubric]\n",
    "        \n",
    "        for annotator in model_order:\n",
    "            if annotator in subset['level_0'].values:\n",
    "                sub = subset[subset['level_0'] == annotator]\n",
    "\n",
    "                if annotator == \"Trained Annotators\":\n",
    "                    annotator = \"Human Scorers\"\n",
    "                \n",
    "                ax.plot(sub['Subsample'], sub['mean'],\n",
    "                        marker='o', label=annotator,\n",
    "                        color=color_map[annotator])\n",
    "                if show_CI:\n",
    "                    ax.fill_between(sub['Subsample'],\n",
    "                                    sub['lower'], sub['upper'],\n",
    "                                    alpha=0.2, color=color_map[annotator])\n",
    "        \n",
    "        # --- Add vertical line for mean \"Min Subsample (%)\" ---\n",
    "        if grouped_summary_df is not None:\n",
    "            print('rubric', rubric, 'codebook', codebook)\n",
    "            try:\n",
    "                min_sub_10_mean = grouped_summary_df.loc[(rubric, codebook), 'Min Subsample (10%)']\n",
    "                min_sub_5_mean = grouped_summary_df.loc[(rubric, codebook), 'Min Subsample (5%)']\n",
    "                print('min_sub 10: ', min_sub_10_mean)\n",
    "                print('min_sub 1: ', min_sub_5_mean)\n",
    "                ax.axvline(x=min_sub_10_mean, color='grey', linestyle='--', linewidth=1.5, label='10% threshold')\n",
    "                ax.text(min_sub_10_mean, 0.85,\n",
    "                        f\"{min_sub_10_mean:.0f}%\", rotation=0,\n",
    "                        va='top', ha='right', color='grey', fontsize=10)\n",
    "                ax.axvline(x=min_sub_5_mean, color='black', linestyle='--', linewidth=1.5, label='5% threshold')\n",
    "                ax.text(min_sub_5_mean, 0.85,\n",
    "                        f\"{min_sub_5_mean:.0f}%\", rotation=0,\n",
    "                        va='top', ha='right', color='black', fontsize=10)\n",
    "            except KeyError:\n",
    "                # No entry for this (rubric, codebook)\n",
    "                pass\n",
    "        \n",
    "        ax.set_title(f\"{rubric}\")\n",
    "        ax.set_xlabel('% of Responses')\n",
    "        ax.set_ylabel('ICC')\n",
    "        ax.grid(True)\n",
    "    \n",
    "    # Legend & title\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, title='Model',\n",
    "               loc='center left', bbox_to_anchor=(0.9, 0.5))\n",
    "    \n",
    "    # fig.suptitle(f'ICC vs Subsample Size ({codebook})', fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0, 0.85, 0.95])  # leave space for legend and title\n",
    "    \n",
    "    plt.savefig(f\"./output/consistency/icc/subsample_{codebook}_interviews-4-50-2.png\",\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff2b05f-af1a-40df-87eb-af06c97a2a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual plots\n",
    "for codebook in ['Zero-Shot', 'Few-Shot', 'CoT']:\n",
    "    subset = agg_results_wide[(agg_results_wide['Codebook'] == codebook)]\n",
    "    plot_icc_subplots(subset, codebook, color_map, model_order, show_CI=True, grouped_summary_df=grouped_summary_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06837030-645a-4f25-8b1c-e4fb59b17b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate across codebook types\n",
    "agg_combined = (\n",
    "    agg_results_wide\n",
    "    .groupby(['Subsample', 'level_0', 'Rubric'])\n",
    "    .agg({\n",
    "        'mean': 'mean',       # average ICC across codebooks\n",
    "        'lower': 'min',       # min lower bound across codebooks\n",
    "        'upper': 'max'        # max upper bound across codebooks\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Call once with the full data\n",
    "plot_icc_subplots(agg_combined, \"Combined\", color_map, model_order, show_CI=False, grouped_summary_df=grouped_summary_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a4832-9bd8-4fbd-9cc0-cafd079fdd67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
