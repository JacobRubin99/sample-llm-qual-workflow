{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f83dc786-8891-48d4-9b6c-36d5d57f9ad2",
   "metadata": {},
   "source": [
    "# Calculate IntraClass Correlations (ICCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b12885-61ed-4e5c-8206-79ff6d21ff4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8057cd9-de36-4375-8589-5c1c1dadfde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c93361e-9442-4cb1-a95f-e2bfaf6146ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49951940-c3c2-46f8-953f-f21e76c2e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rubric item categories\n",
    "reasoning_cols = [\"Engagement with Evidence\", \"Goal Orientation\",\n",
    "                  \"Collaborative Decision-Making\", \"Strategic Planning\"]\n",
    "hire_cols = [\n",
    "    \"Cultural Responsiveness\", \"Parent/Community Engagement\", \"Academic Achievement\",\n",
    "    \"Candidate Experience/Expertise\", \"Evaluation\", \"School Culture Fit\"\n",
    "]\n",
    "\n",
    "rubric_items = reasoning_cols + hire_cols\n",
    "\n",
    "model_scorers = ['gpt-4.1-mini', 'gpt-5', \"us.anthropic.claude-sonnet-4-20250514-v1:0\"]\n",
    "human_scorers = ['JR', 'MF']\n",
    "human_avg_score = ['human avg']\n",
    "\n",
    "valid_scorer_names = {\n",
    "    \"us.anthropic.claude-sonnet-4-20250514-v1:0\": 'Claude Sonnet 4',\n",
    "    \"gpt-4.1-mini\": \"GPT-4.1 mini\",\n",
    "    \"gpt-5\": \"GPT-5\",\n",
    "    \"human\": \"Trained Annotators\",\n",
    "}\n",
    "\n",
    "codebook_types = [\"zero-shot\", \"examples\", \"cot_examples\"]\n",
    "\n",
    "codebook_type_names = {\n",
    "        \"zero-shot\": \"Zero-Shot\",\n",
    "        \"examples\": \"Few-Shot\",\n",
    "        \"cot_examples\": \"CoT\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80309cd-8e16-4c05-8e77-11d336502735",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0842a281-6ef5-4cbb-8035-89b627df4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for ctype in codebook_types:\n",
    "    df = pd.read_csv(\n",
    "        f\"./scores/filtered/{ctype}_df.csv\", \n",
    "        header=0, index_col=0\n",
    "    )\n",
    "    df[\"codebook type\"] = ctype\n",
    "    df.loc[df[\"Scorer Name\"].isin(human_scorers), \"codebook type\"] = \"human\"\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate codebook files\n",
    "scores_df = pd.concat(dfs, ignore_index=True).drop_duplicates()\n",
    "\n",
    "# Add \"Scoring Group\" column\n",
    "scores_df = scores_df[scores_df[\"Scorer Name\"] != \"human avg\"]\n",
    "scores_df.loc[scores_df[\"Scorer Name\"].isin(model_scorers), \"Scoring Group\"] = \"model\"\n",
    "scores_df.loc[scores_df[\"Scorer Name\"].isin(human_scorers), \"Scoring Group\"] = \"human\"\n",
    "\n",
    "scores_df['Scorer Name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3e3ce1-d957-48ba-bdcc-a89542ef4074",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Calculate Human-Human ICC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3064973-6701-40b1-96e5-b0975d76c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_human_icc(scores_df):\n",
    "    \"\"\"\n",
    "    Compute ICC(2,1) for human scorers per rubric item, \n",
    "    and average ICCs for Reasoning and Hiring Priorities categories.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    scores_df : pd.DataFrame\n",
    "        DataFrame containing scores with columns 'record_id', 'Scorer Name', 'Scoring Group', and rubric items\n",
    "    rubric_items : list\n",
    "        List of all rubric items to compute ICC for\n",
    "    reasoning_cols : list\n",
    "        List of rubric items corresponding to Reasoning\n",
    "    hire_cols : list\n",
    "        List of rubric items corresponding to Hiring Priorities\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    icc_human_df : pd.DataFrame\n",
    "        DataFrame with ICC(2,1) and 95% CI for each rubric item\n",
    "    human_iccs : dict\n",
    "        Dictionary with averaged ICCs for 'Reasoning' and 'Hiring Priorities'\n",
    "    \"\"\"\n",
    "    \n",
    "    icc_human_results = []\n",
    "\n",
    "    for item in rubric_items:\n",
    "        # Filter for human scorers and the current item\n",
    "        group_human = scores_df[scores_df[\"Scoring Group\"] == \"human\"][[\"record_id\", \"Scorer Name\", item]].dropna()\n",
    "        group_human_long = group_human.rename(columns={item: \"score\", \"Scorer Name\": \"rater\"})\n",
    "        \n",
    "        # Only compute ICC if enough raters and records\n",
    "        if group_human_long[\"rater\"].nunique() > 1 and group_human_long[\"record_id\"].nunique() > 1:\n",
    "            icc_human = pg.intraclass_corr(\n",
    "                data=group_human_long, targets='record_id', raters='rater', ratings='score'\n",
    "            )\n",
    "            icc2 = icc_human[icc_human[\"Type\"] == \"ICC2\"]\n",
    "            if not icc2.empty:\n",
    "                icc_human_results.append({\n",
    "                    \"Rubric Item\": item,\n",
    "                    \"ICC(2,1)\": icc2[\"ICC\"].values[0],\n",
    "                    \"CI95%\": icc2[\"CI95%\"].values[0]\n",
    "                })\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    icc_human_df = pd.DataFrame(icc_human_results)\n",
    "\n",
    "    \n",
    "    # Compute averages by category\n",
    "    icc_human_reason_avg = icc_human_df.loc[\n",
    "        icc_human_df[\"Rubric Item\"].isin(reasoning_cols), \"ICC(2,1)\"\n",
    "    ].mean()\n",
    "    \n",
    "    icc_human_hire_avg = icc_human_df.loc[\n",
    "        icc_human_df[\"Rubric Item\"].isin(hire_cols), \"ICC(2,1)\"\n",
    "    ].mean()\n",
    "    \n",
    "    human_iccs = {\n",
    "        \"Reasoning\": icc_human_reason_avg,\n",
    "        \"Hiring Priorities\": icc_human_hire_avg\n",
    "    }\n",
    "    \n",
    "    return icc_human_df, human_iccs\n",
    "\n",
    "icc_human_df, human_iccs = compute_human_icc(scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae3ecd-6a2d-4e59-a466-b1bfad629f11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Calculate Model-Human ICC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914bf18e-2760-4763-b7a7-8dc62cdd4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_icc(scores_df):\n",
    "    \"\"\"\n",
    "    Compute ICC(2,1) for model raters against human averages, grouped by codebook type and rubric item.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scores_df : pd.DataFrame\n",
    "        DataFrame containing scores with columns 'record_id', 'Scorer Name', 'Scoring Group', 'codebook type', and rubric items\n",
    "    rubric_items : list\n",
    "        List of all rubric items to compute ICC for\n",
    "    codebook_types : list\n",
    "        List of codebook types to compute ICC for\n",
    "    valid_scorer_names : dict, optional\n",
    "        Mapping to rename raters\n",
    "    codebook_type_names : dict, optional\n",
    "        Mapping to rename codebook types\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    icc_summary_df : pd.DataFrame\n",
    "        DataFrame with ICC(2,1) and 95% CI per rater, rubric item, and codebook type\n",
    "    \"\"\"\n",
    "\n",
    "    all_icc_results = []\n",
    "\n",
    "    # Loop through each codebook type\n",
    "    for ctype in codebook_types:\n",
    "        df_sub = scores_df[(scores_df[\"codebook type\"] == ctype) | (scores_df[\"codebook type\"] == \"human\")]\n",
    "\n",
    "        # Loop through each rubric item\n",
    "        for item in rubric_items:\n",
    "\n",
    "            # Group Human averages per subject\n",
    "            group_human_means = (\n",
    "                df_sub[df_sub[\"Scoring Group\"] == \"human\"]\n",
    "                .groupby(\"record_id\")[item]\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                .rename(columns={item: \"group_human_avg\"})\n",
    "            )\n",
    "\n",
    "            # Group Model individual rater scores\n",
    "            group_model = df_sub[df_sub[\"Scoring Group\"] == \"model\"][[\"record_id\", \"Scorer Name\", item]].dropna()\n",
    "\n",
    "            # Merge Group Model with Group Human averages\n",
    "            merged = pd.merge(group_model, group_human_means, on=\"record_id\", how=\"inner\").dropna()\n",
    "\n",
    "            for rater in merged[\"Scorer Name\"].unique():\n",
    "                rater_data = merged[merged[\"Scorer Name\"] == rater][\n",
    "                    [\"record_id\", item, \"group_human_avg\"]\n",
    "                ].dropna()\n",
    "\n",
    "                # Melt to long format for pingouin\n",
    "                icc_data = rater_data.melt(\n",
    "                    id_vars=\"record_id\",\n",
    "                    value_vars=[item, \"group_human_avg\"],\n",
    "                    var_name=\"rater\",\n",
    "                    value_name=\"score\"\n",
    "                )\n",
    "\n",
    "                icc = pg.intraclass_corr(\n",
    "                    data=icc_data, targets=\"record_id\", raters=\"rater\", ratings=\"score\"\n",
    "                )\n",
    "                icc2 = icc[icc[\"Type\"] == \"ICC2\"]\n",
    "\n",
    "                all_icc_results.append({\n",
    "                    \"Codebook Type\": ctype,\n",
    "                    \"Rubric Item\": item,\n",
    "                    \"Rater\": rater,\n",
    "                    \"ICC(2,1)\": icc2[\"ICC\"].values[0],\n",
    "                    \"CI95%\": icc2[\"CI95%\"].values[0]\n",
    "                })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    icc_summary_df = pd.DataFrame(all_icc_results)\n",
    "\n",
    "    # Rename raters and codebook types\n",
    "    if valid_scorer_names:\n",
    "        icc_summary_df[\"Rater\"] = icc_summary_df[\"Rater\"].replace(valid_scorer_names)\n",
    "    if codebook_type_names:\n",
    "        icc_summary_df[\"Codebook Type\"] = icc_summary_df[\"Codebook Type\"].replace(codebook_type_names)\n",
    "\n",
    "    return icc_summary_df\n",
    "\n",
    "icc_summary_df = compute_model_icc(scores_df)\n",
    "\n",
    "# Save combined ICC results\n",
    "icc_summary_df.to_csv(\"output/consistency/icc/all_ICC_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
