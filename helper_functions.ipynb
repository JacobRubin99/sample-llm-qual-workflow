{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33ad19d-56e5-44b1-b321-46e1a25a35ba",
   "metadata": {},
   "source": [
    "## Helper Functions (for Step 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc273d5-1894-4e54-9f99-1a0e44147ea7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c723d-c685-4ecb-9541-fc2b969057e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1237723d-11ab-409b-a498-04093f7a1165",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Define Global Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052528b5-274d-40b4-b800-ab0e1d9d15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_cols = [\"Engagement with Evidence\", \"Goal Orientation\",\n",
    "                  \"Collaborative Decision-Making\", \"Strategic Planning\"]\n",
    "hire_cols = [\"Cultural Responsiveness\", \"Parent/Community Engagement\", \"Academic Achievement\", \n",
    "                \"Candidate Experience/Expertise\", \"Evaluation\", \"School Culture Fit\"]\n",
    "\n",
    "cols =  reasoning_cols + hire_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c214b7-5747-44b0-aa3b-4db1d777b8b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions: Concatenate LLM Scores into single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfadd4db-b645-4215-9cbc-88f035806094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_raw_scores(model, path):\n",
    "    csv_files = glob.glob(f'{path}/{model}/{model}_*.csv') \n",
    "    \n",
    "    # Read and concatenate into one DataFrame\n",
    "    df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "    \n",
    "    # Add a column with the number of iterations per record_id: \n",
    "    df['iteration'] = df.groupby(['record_id', \"Scorer Name\"]).cumcount() + 1\n",
    "    df.drop(\"call_number\", axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5afb65-2376-4643-bd10-08a876a9cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_vals_numeric(training_df):\n",
    "    # Convert column values to numeric\n",
    "    training_df.loc[:, cols] = training_df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Convert \"record_id\" column to numeric\n",
    "    training_df.loc[:, [\"record_id\"]] = training_df[[\"record_id\"]].apply(pd.to_numeric)\n",
    "    \n",
    "    return training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94ce02-c2f3-47e0-9bc9-e0643941beab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions: Handle Invalid Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b331bf7a-5cb3-44ae-88fb-1ea3a75d9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_invalid_values(training_df):\n",
    " \n",
    "    # Convert invalid hire col values to NaN\n",
    "    training_df[hire_cols] = training_df[hire_cols].map(lambda x: x if x in [0, 1] else np.nan)\n",
    "    \n",
    "    # Convert invalid reasoning col values to NaN\n",
    "    training_df[reasoning_cols] = training_df[reasoning_cols].map(\n",
    "        lambda x: x if pd.notna(x) and isinstance(x, (int, float, np.integer, np.floating)) and x == int(x) and 1 <= x <= 4 else np.nan\n",
    "    )\n",
    "    \n",
    "    return training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdfd30a-7944-490b-8f49-d657ce8a3d22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions: Concatenate Trained Annotators Dataframe with LLM Scoring Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce61943-6f25-4638-a6a8-8992e51512cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Human Coders dataframe\n",
    "def load_human_coders():\n",
    "    # Read in scorer #1 responses\n",
    "    mf_df = pd.read_excel('../../data/Interviews/MF Scoring.xlsx')\n",
    "    mf_df = mf_df[0:]\n",
    "    mf_df = mf_df.fillna(0)\n",
    "\n",
    "    # Read in scorer #2 responses\n",
    "    jr_df = pd.read_excel('../../data/Interviews/JR Scoring.xlsx')\n",
    "    jr_df = jr_df[0:]\n",
    "    jr_df = jr_df.fillna(0)\n",
    "    \n",
    "    human_df = pd.concat([mf_df, jr_df])\n",
    "\n",
    "    return human_df\n",
    "\n",
    "# Create avg. human coder scoring df\n",
    "def create_avg_human_df(human_df):\n",
    "\n",
    "    human_avg_df = human_df[[\"record_id\"] + cols].groupby([\"record_id\"], as_index=False).mean()\n",
    "    human_avg_df[cols] = np.floor(human_avg_df[cols] + 0.5)\n",
    "    \n",
    "    human_avg_df[\"Scorer Name\"] = \"human avg\"\n",
    "\n",
    "    return human_avg_df\n",
    "\n",
    "# Concat LLM + Human Coder scoring dataframe\n",
    "def concat_llm_human_df(training_df, rounded=True, is_training_data=False):\n",
    "\n",
    "    # Concat human coders dataframes\n",
    "    human_df = load_human_coders()\n",
    "    human_avg_df = create_avg_human_df(human_df)\n",
    "    \n",
    "    # Use training_df averages\n",
    "    training_avg_df = training_df[[\"record_id\", \"Scorer Name\"] + cols].groupby([\"record_id\", \"Scorer Name\"], as_index=False).mean()\n",
    "    if rounded:\n",
    "        training_avg_df[cols] = np.floor(training_avg_df[cols] + 0.5)\n",
    "\n",
    "    # Append responses in single dataframe\n",
    "    concat_df = pd.concat([training_avg_df, human_df, human_avg_df])\n",
    "    if is_training_data:\n",
    "        concat_df = concat_df[concat_df['record_id'].isin(training_ids)]\n",
    "    else: \n",
    "        concat_df = concat_df[~concat_df['record_id'].isin(training_ids)]\n",
    "    \n",
    "    # Only keep columns we're interested in\n",
    "    concat_df = concat_df[[\"record_id\", \"Scorer Name\"] + cols]\n",
    "    concat_df = concat_df.sort_values(by=['record_id', 'Scorer Name'])\n",
    "    \n",
    "    filtered_df = concat_df\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f5e279-6a60-402d-aacd-afa90cfd0ee3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions: Evaluate Compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39c78e-339a-4083-bf4c-516f098cf555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compliance Checks\n",
    "#     - Failed calls: How many calls failed? \n",
    "#     - ID hallucinations: Are 55 respondents scored? Any fake ones?\n",
    "#     - Scorer Names: Are the only scorer names clade, gpt, etc.?\n",
    "#     - Reasoning scores: Bounded by 1-5? Missingness?\n",
    "#     - Preferences: Bounded by 0-2? Missingness?\n",
    "#     - Hire Factors: Bounded by 0-1? Missingness?\n",
    "\n",
    "def run_compliance_checks(df, valid_ids, codebook_type=\"zero-shot\"):\n",
    "    # Define expected scorer names\n",
    "    valid_scorer_names = {\n",
    "        \"us.anthropic.claude-sonnet-4-20250514-v1:0\": \"sonnet-4\",\n",
    "        \"gpt-4.1-mini\": \"gpt-41-mini\",\n",
    "        \"gpt-5\": \"gpt-5\",\n",
    "    }\n",
    "    \n",
    "    # Initialize result structure\n",
    "    results = {\n",
    "        \"gpt-5\": [],\n",
    "        \"sonnet-4\": [],\n",
    "        \"gpt-41-mini\": []\n",
    "    }\n",
    "\n",
    "    # Group dataframe by scorer name\n",
    "    grouped = df.groupby(\"Scorer Name\")\n",
    "\n",
    "    for scorer_fullname, group in grouped:\n",
    "        scorer_shortname = valid_scorer_names.get(scorer_fullname)\n",
    "        if scorer_shortname is None:\n",
    "            continue  # Skip any unknown scorer names\n",
    "\n",
    "        # Check 1: Only valid scorer names present\n",
    "        all_names_valid = set(df[\"Scorer Name\"]).issubset(valid_scorer_names.keys())\n",
    "        # print(df[\"Scorer Name\"].unique())\n",
    "        results[scorer_shortname].append(all_names_valid)\n",
    "\n",
    "        # Check 2: Proportion of rows (93 responses)\n",
    "        num_respondents = len(valid_ids)\n",
    "        row_count_proportion = (len(group) / (num_respondents * 10))*100\n",
    "        results[scorer_shortname].append(row_count_proportion)\n",
    "\n",
    "        # Check 3: Number of invalid record_ids\n",
    "        invalid_record_ids = (~group[\"record_id\"].isin(valid_ids)).sum()\n",
    "        results[scorer_shortname].append(invalid_record_ids)\n",
    "\n",
    "        # Check 4: Reasoning (1 to 4, missingness)\n",
    "        reasoning_series = group[reasoning_cols]\n",
    "        missing_reasoning = reasoning_series.isna().any(axis=1)\n",
    "        valid_reasoning = reasoning_series.apply(lambda col: col.map(lambda x: x in [1, 2, 3, 4])).all(axis=1)\n",
    "        invalid_reasoning = ~valid_reasoning & ~missing_reasoning\n",
    "\n",
    "        total = len(reasoning_series)\n",
    "        results[scorer_shortname].append(valid_reasoning.sum() / total)\n",
    "        results[scorer_shortname].append(missing_reasoning.sum() / total)\n",
    "        results[scorer_shortname].append(invalid_reasoning.sum() / total)\n",
    "\n",
    "        # Check 5: Hiring columns (0 or 1, missingness)\n",
    "        hire_cols = [\n",
    "            \"Cultural Responsiveness\",\n",
    "            \"Parent/Community Engagement\",\n",
    "            \"Academic Achievement\",\n",
    "            \"Candidate Experience/Expertise\",\n",
    "            \"Evaluation\",\n",
    "            \"School Culture Fit\"\n",
    "        ]\n",
    "        hire_data = group[hire_cols]\n",
    "        missing_hire = hire_data.isna().any(axis=1)\n",
    "        valid_hire = hire_data.apply(lambda col: col.map(lambda x: x in [0, 1])).all(axis=1)\n",
    "        invalid_hire = ~(valid_hire | missing_hire)\n",
    "\n",
    "        total = len(hire_data)\n",
    "        results[scorer_shortname].append(valid_hire.sum() / total)\n",
    "        results[scorer_shortname].append(missing_hire.sum() / total)\n",
    "        results[scorer_shortname].append(invalid_hire.sum() / total)\n",
    "\n",
    "\n",
    "        compliance_df = pd.DataFrame.from_dict(results, orient=\"index\", columns=[\n",
    "            \"Valid Scorer Names\",\n",
    "            \"Row Count Proportion\",\n",
    "            \"ID Hallucinations\",\n",
    "            # Check 4\n",
    "            \"Reasoning: Valid\",\n",
    "            \"Reasoning: Missing\",\n",
    "            \"Reasoning: Invalid\",\n",
    "            # Check 5\n",
    "            \"Hire Factors: Valid\",\n",
    "            \"Hire Factors: Missing\",\n",
    "            \"Hire Factors: Invalid\"\n",
    "        ])\n",
    "\n",
    "    compliance_df = compliance_df.round(3)\n",
    "    compliance_df.to_csv(f\"output/compliance/{codebook_type}_compliance_df.csv\")\n",
    "    \n",
    "    return compliance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebee3e7-2754-459b-afa2-52f901383fde",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions: Evaluate Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4008df-9fe9-4f1e-96d1-8299d4c67751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_variation(df):\n",
    "    # Calculate means and std\n",
    "    results = []\n",
    "    \n",
    "    for col in cols:\n",
    "        # Calculate mean and standard deviation by record_id\n",
    "        consistency_df = df.groupby(\"record_id\")[col].agg(['mean', 'std']).reset_index()\n",
    "    \n",
    "        consistency_df['std'] = consistency_df['std'].fillna(0)\n",
    "        # Calculate average std. dev. and average CV\n",
    "        average_std = consistency_df['std'].mean()\n",
    "        \n",
    "        # Calculate overall mean and std_dev of rubric item\n",
    "        overall_mean = df[col].mean()\n",
    "        overall_std = df[col].std()\n",
    "        \n",
    "        # Append results\n",
    "        results.append({\n",
    "            \"Rubric Item\": col,\n",
    "            \"Overall Mean\": overall_mean,\n",
    "            \"Overall SD (across resp.)\": overall_std,\n",
    "            \"Avg. SD (within resp.)\": average_std\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    summary_df = pd.DataFrame(results)\n",
    "\n",
    "    # @TODO careful of rounding here - pandas turns 1.5 into 1\n",
    "    summary_df = summary_df.round(3)\n",
    "    return summary_df\n",
    "\n",
    "def save_variation_df(training_df, codebook_type=\"zero-shot\"):\n",
    "\n",
    "    # Define models and their display names\n",
    "    model_labels = {\n",
    "        \"gpt-5\": \"GPT-5\",\n",
    "        \"sonnet-4\": \"Sonnet 4\",\n",
    "        \"gpt-41-mini\": \"GPT 41 Mini\"\n",
    "    }\n",
    "    \n",
    "    # Compute variation for each model\n",
    "    variation_dict = {\n",
    "        label: calc_variation(training_df[training_df[\"Scorer Name\"] == engines[key]])\n",
    "        for key, label in model_labels.items()\n",
    "    }\n",
    "    \n",
    "    # Combine all into one dataframe\n",
    "    variation_df = pd.concat(variation_dict, axis=1)\n",
    "\n",
    "    variation_df.to_csv(f\"output/variation/{codebook_type}_variation_df.csv\")\n",
    "    return variation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc5d3d-9349-4bed-9e61-d9d525c2a217",
   "metadata": {},
   "source": [
    "### Functions: Evaluate Uncertainty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e2ceb-1d84-4442-9d87-2f0f6d352f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_testset_df(codebook_types, training_ids):\n",
    "    all_testsets = []\n",
    "\n",
    "    for codebook_type in codebook_types:\n",
    "        gpt_5_df = concat_raw_scores(\"gpt-5\", path=f\"scores/{codebook_type}\")\n",
    "        sonnet_4_df = concat_raw_scores(\"sonnet-4\", path=f\"scores/{codebook_type}\")\n",
    "        gpt_41_mini_df = concat_raw_scores(\"gpt-41-mini\", path=f\"scores/{codebook_type}\")\n",
    "\n",
    "        testing_df = pd.concat([gpt_5_df, sonnet_4_df, gpt_41_mini_df])\n",
    "        testing_df = testing_df[~testing_df[\"record_id\"].isin(training_ids)]\n",
    "\n",
    "        testing_df = convert_vals_numeric(testing_df)\n",
    "        testing_df = handle_invalid_values(testing_df)\n",
    "\n",
    "        testing_df[\"codebook_type\"] = codebook_type\n",
    "        all_testsets.append(testing_df)\n",
    "\n",
    "    testset_df = pd.concat(all_testsets, ignore_index=True)\n",
    "\n",
    "    return testset_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43132cd8-49c1-48a3-8937-d4d2cbce3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy_pivot(\n",
    "    testset_df,\n",
    "    rubric_items,\n",
    "    reasoning_cols,\n",
    "    hire_cols,\n",
    "    scorer_name_map,\n",
    "    scorer_order,\n",
    "):\n",
    "    # Ensure numeric\n",
    "    testset_df[rubric_items] = testset_df[rubric_items].apply(\n",
    "        pd.to_numeric, errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    def compute_entropy(scores):\n",
    "        scores = scores.dropna()\n",
    "        if len(scores) == 0:\n",
    "            return np.nan\n",
    "        probs = scores.value_counts(normalize=True).values\n",
    "        return -np.sum(probs * np.log(probs))\n",
    "\n",
    "    long_df = testset_df.melt(\n",
    "        id_vars=[\"record_id\", \"Scorer Name\", \"codebook_type\"],\n",
    "        value_vars=rubric_items,\n",
    "        var_name=\"Rubric Item\",\n",
    "        value_name=\"Score\",\n",
    "    )\n",
    "\n",
    "    entropy_df = (\n",
    "        long_df\n",
    "        .groupby(\n",
    "            [\"record_id\", \"Scorer Name\", \"Rubric Item\", \"codebook_type\"]\n",
    "        )[\"Score\"]\n",
    "        .apply(compute_entropy)\n",
    "        .reset_index(name=\"Entropy\")\n",
    "    )\n",
    "\n",
    "    # Max entropy per item\n",
    "    max_entropy_map = {item: np.log(4) for item in reasoning_cols}\n",
    "    max_entropy_map.update({item: np.log(2) for item in hire_cols})\n",
    "\n",
    "    entropy_df[\"Normalized_Entropy\"] = entropy_df.apply(\n",
    "        lambda r: r[\"Entropy\"] / max_entropy_map[r[\"Rubric Item\"]],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    avg_entropy_df = (\n",
    "        entropy_df\n",
    "        .groupby([\"Scorer Name\", \"Rubric Item\", \"codebook_type\"])[\"Normalized_Entropy\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"Normalized_Entropy\": \"Entropy\"})\n",
    "    )\n",
    "\n",
    "    avg_entropy_df[\"Category\"] = np.select(\n",
    "        [\n",
    "            avg_entropy_df[\"Rubric Item\"].isin(reasoning_cols),\n",
    "            avg_entropy_df[\"Rubric Item\"].isin(hire_cols),\n",
    "        ],\n",
    "        [\"Reasoning\", \"Hiring\"],\n",
    "        default=np.nan,\n",
    "    )\n",
    "\n",
    "    avg_entropy_df_by_category = (\n",
    "        avg_entropy_df\n",
    "        .groupby([\"Scorer Name\", \"Category\", \"codebook_type\"])[\"Entropy\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    avg_entropy_df_by_category[\"Scorer Name\"] = (\n",
    "        avg_entropy_df_by_category[\"Scorer Name\"]\n",
    "        .replace(scorer_name_map)\n",
    "    )\n",
    "\n",
    "    pivot_df = (\n",
    "        avg_entropy_df_by_category\n",
    "        .pivot_table(\n",
    "            index=\"Scorer Name\",\n",
    "            columns=[\"Category\", \"codebook_type\"],\n",
    "            values=\"Entropy\",\n",
    "        )\n",
    "        .reindex(scorer_order)\n",
    "        .round(2)\n",
    "    )\n",
    "    pivot_df.to_csv(\"output/uncertainty/entropy_scores.csv\")\n",
    "    return pivot_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
